{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd5b6f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-24T17:16:36.072653Z",
     "iopub.status.busy": "2025-11-24T17:16:36.072420Z",
     "iopub.status.idle": "2025-11-24T17:18:00.522708Z",
     "shell.execute_reply": "2025-11-24T17:18:00.521511Z"
    },
    "papermill": {
     "duration": 84.456248,
     "end_time": "2025-11-24T17:18:00.524364",
     "exception": false,
     "start_time": "2025-11-24T17:16:36.068116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet -q\n",
    "!pip install category_encoders -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a65328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:00.564424Z",
     "iopub.status.busy": "2025-11-24T17:18:00.564146Z",
     "iopub.status.idle": "2025-11-24T17:18:11.456071Z",
     "shell.execute_reply": "2025-11-24T17:18:11.455446Z"
    },
    "papermill": {
     "duration": 10.913433,
     "end_time": "2025-11-24T17:18:11.457597",
     "exception": false,
     "start_time": "2025-11-24T17:18:00.544164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd73f383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:11.498102Z",
     "iopub.status.busy": "2025-11-24T17:18:11.497624Z",
     "iopub.status.idle": "2025-11-24T17:18:30.777931Z",
     "shell.execute_reply": "2025-11-24T17:18:30.777047Z"
    },
    "papermill": {
     "duration": 19.302223,
     "end_time": "2025-11-24T17:18:30.779451",
     "exception": false,
     "start_time": "2025-11-24T17:18:11.477228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading the 2016 train + properties\n",
    "train2016 = pd.read_csv('/kaggle/input/zillow-prize-1/train_2016_v2.csv')\n",
    "props2016 = pd.read_csv('/kaggle/input/zillow-prize-1/properties_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d776c2cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:30.817385Z",
     "iopub.status.busy": "2025-11-24T17:18:30.816829Z",
     "iopub.status.idle": "2025-11-24T17:18:48.336063Z",
     "shell.execute_reply": "2025-11-24T17:18:48.335413Z"
    },
    "papermill": {
     "duration": 17.539655,
     "end_time": "2025-11-24T17:18:48.337651",
     "exception": false,
     "start_time": "2025-11-24T17:18:30.797996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loadin the 2017 train + properties (for extra data)\n",
    "train2017 = pd.read_csv('/kaggle/input/zillow-prize-1/train_2017.csv')\n",
    "props2017 = pd.read_csv('/kaggle/input/zillow-prize-1/properties_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a19130f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:48.375139Z",
     "iopub.status.busy": "2025-11-24T17:18:48.374894Z",
     "iopub.status.idle": "2025-11-24T17:18:49.633062Z",
     "shell.execute_reply": "2025-11-24T17:18:49.632449Z"
    },
    "papermill": {
     "duration": 1.278254,
     "end_time": "2025-11-24T17:18:49.634496",
     "exception": false,
     "start_time": "2025-11-24T17:18:48.356242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Merging 2016\n",
    "df2016 = train2016.merge(props2016, on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac001417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:49.672376Z",
     "iopub.status.busy": "2025-11-24T17:18:49.671836Z",
     "iopub.status.idle": "2025-11-24T17:18:50.864458Z",
     "shell.execute_reply": "2025-11-24T17:18:50.863745Z"
    },
    "papermill": {
     "duration": 1.21293,
     "end_time": "2025-11-24T17:18:50.866109",
     "exception": false,
     "start_time": "2025-11-24T17:18:49.653179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Merging 2017(fill missing from 2016 props where possible)\n",
    "df2017 = train2017.merge(props2017, on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86404e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:50.904229Z",
     "iopub.status.busy": "2025-11-24T17:18:50.903587Z",
     "iopub.status.idle": "2025-11-24T17:18:52.623053Z",
     "shell.execute_reply": "2025-11-24T17:18:52.622477Z"
    },
    "papermill": {
     "duration": 1.74008,
     "end_time": "2025-11-24T17:18:52.624456",
     "exception": false,
     "start_time": "2025-11-24T17:18:50.884376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For 2017 missing props filling from 2016 if same parcel\n",
    "missing_2017 = df2017[df2017['latitude'].isna()]  # Example: if key features missing\n",
    "df2017 = df2017.combine_first(missing_2017.merge(props2016, on='parcelid', how='left', suffixes=('', '_2016')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab265d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:52.662111Z",
     "iopub.status.busy": "2025-11-24T17:18:52.661856Z",
     "iopub.status.idle": "2025-11-24T17:18:52.831311Z",
     "shell.execute_reply": "2025-11-24T17:18:52.830456Z"
    },
    "papermill": {
     "duration": 0.189833,
     "end_time": "2025-11-24T17:18:52.832800",
     "exception": false,
     "start_time": "2025-11-24T17:18:52.642967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combining both years\n",
    "df = pd.concat([df2016, df2017], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b93636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:52.871138Z",
     "iopub.status.busy": "2025-11-24T17:18:52.870695Z",
     "iopub.status.idle": "2025-11-24T17:18:52.880085Z",
     "shell.execute_reply": "2025-11-24T17:18:52.879302Z"
    },
    "papermill": {
     "duration": 0.029915,
     "end_time": "2025-11-24T17:18:52.881202",
     "exception": false,
     "start_time": "2025-11-24T17:18:52.851287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cliping the outliers (important for low error)\n",
    "df['logerror'] = df['logerror'].clip(lower=-0.4, upper=0.419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93debf7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:52.918392Z",
     "iopub.status.busy": "2025-11-24T17:18:52.917964Z",
     "iopub.status.idle": "2025-11-24T17:18:53.120675Z",
     "shell.execute_reply": "2025-11-24T17:18:53.120037Z"
    },
    "papermill": {
     "duration": 0.22244,
     "end_time": "2025-11-24T17:18:53.121726",
     "exception": false,
     "start_time": "2025-11-24T17:18:52.899286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (167888, 117)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Combined data shape: {df.shape}\")\n",
    "del train2016, props2016, train2017, props2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf20d441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:53.161402Z",
     "iopub.status.busy": "2025-11-24T17:18:53.161146Z",
     "iopub.status.idle": "2025-11-24T17:18:55.754555Z",
     "shell.execute_reply": "2025-11-24T17:18:55.753967Z"
    },
    "papermill": {
     "duration": 2.613929,
     "end_time": "2025-11-24T17:18:55.755689",
     "exception": false,
     "start_time": "2025-11-24T17:18:53.141760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Checking columns after merge...\n",
      "2016 Merged Columns (first 10): ['parcelid', 'logerror', 'transactiondate', 'airconditioningtypeid', 'architecturalstyletypeid', 'basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingclasstypeid', 'buildingqualitytypeid']\n",
      "2017 Merged Columns (first 10): ['airconditioningtypeid', 'airconditioningtypeid_2016', 'architecturalstyletypeid', 'architecturalstyletypeid_2016', 'assessmentyear', 'assessmentyear_2016', 'basementsqft', 'basementsqft_2016', 'bathroomcnt', 'bathroomcnt_2016']\n",
      "Total unique columns in combined df: 117\n",
      "Missing columns (ignored): ['calculatedbedroomnbr', 'calculatedfinishedsquarefeet6', 'calculatedfinishedsquarefeet12', 'calculatedfinishedsquarefeet13', 'calculatedfinishedsquarefeet15', 'calculatedparkingnbr', 'cityid', 'coolingtypeid', 'countyuse1code', 'heatingtypeid', 'landmarkpoint1', 'landmarkpoint2', 'landmarkpoint3', 'landmarkpoint4', 'utilitytype', 'taxassessedvalue', 'taxlien', 'taxassessorvalue', 'taxassessedyear']\n",
      "Using 51 columns instead.\n",
      "Checking for duplicate columns...\n",
      "Found duplicate columns: ['numberofstories']\n",
      "Final columns count: 58\n",
      "Train shape: (134310, 55), Test shape: (33578, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Feature Engineering (Updated - Safe Column Filtering)\n",
    "print(\"Debug: Checking columns after merge...\")\n",
    "\n",
    "# First, print actual columns in merged df to debug\n",
    "print(\"2016 Merged Columns (first 10):\", df2016.columns[:10].tolist())\n",
    "print(\"2017 Merged Columns (first 10):\", df2017.columns[:10].tolist())\n",
    "print(f\"Total unique columns in combined df: {len(df.columns)}\")\n",
    "\n",
    "# Useful columns list (same as before, but now we'll filter safe way)\n",
    "useful_cols = [\n",
    "    'parcelid', 'logerror', 'transactiondate', 'airconditioningtypeid', 'architecturalstyletypeid',\n",
    "    'basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingclasstypeid', 'buildingqualitytypeid',\n",
    "    'calculatedbathnbr', 'calculatedbedroomnbr', 'calculatedfinishedsquarefeet',\n",
    "    'calculatedfinishedsquarefeet6', 'calculatedfinishedsquarefeet12', 'calculatedfinishedsquarefeet13',\n",
    "    'calculatedfinishedsquarefeet15', 'calculatedparkingnbr', 'cityid', 'coolingtypeid',\n",
    "    'countyuse1code', 'decktypeid', 'finishedfloor1squarefeet', 'finishedsquarefeet6',\n",
    "    'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15', 'fips', 'fireplacecnt',\n",
    "    'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'hashottuborspa', 'heatingtypeid',\n",
    "    'landmarkpoint1', 'landmarkpoint2', 'landmarkpoint3', 'landmarkpoint4', 'latitude',\n",
    "    'longitude', 'lotsizesquarefeet', 'numberofstories', 'poolcnt', 'pooltypeid10',\n",
    "    'pooltypeid2', 'pooltypeid7', 'propertylandusetypeid', 'propertyzoningdesc',\n",
    "    'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood',\n",
    "    'regionidzip', 'roomcnt', 'threequarterbathnbr', 'typeconstructiontypeid',\n",
    "    'unitcnt', 'utilitytype', 'yardbuildingsqft17', 'yearbuilt', 'numberofstories',\n",
    "    'fireplaceflag', 'taxamount', 'taxassessedvalue', 'taxdelinquencyflag', 'taxdelinquencyyear',\n",
    "    'taxlien', 'assessmentyear', 'taxassessorvalue', 'taxassessedyear'\n",
    "]\n",
    "\n",
    "#SAFE FILTER: Only keep columns that actually exist in df to avoid key error\n",
    "existing_cols = [col for col in useful_cols if col in df.columns]\n",
    "missing_cols = [col for col in useful_cols if col not in df.columns]\n",
    "print(f\"Missing columns (ignored): {missing_cols}\")  # Yeh print hoga, e.g., ['landmarkpoint1', ...] if any\n",
    "print(f\"Using {len(existing_cols)} columns instead.\")\n",
    "\n",
    "df = df[existing_cols].copy()  # Now safe - no error!\n",
    "\n",
    "#Rest of engineering same\n",
    "#Date features\n",
    "df['transactiondate'] = pd.to_datetime(df['transactiondate'])\n",
    "df['transaction_month'] = df['transactiondate'].dt.month\n",
    "df['transaction_year'] = df['transactiondate'].dt.year\n",
    "df['transaction_day'] = df['transactiondate'].dt.day\n",
    "\n",
    "#Engineering new features\n",
    "if 'yearbuilt' in df.columns:\n",
    "    df['age'] = 2017 - df['yearbuilt'].fillna(0)\n",
    "if all(col in df.columns for col in ['taxamount', 'calculatedfinishedsquarefeet']):\n",
    "    df['tax_per_sqft'] = df['taxamount'] / (df['calculatedfinishedsquarefeet'] + 1)\n",
    "if all(col in df.columns for col in ['bedroomcnt', 'bathroomcnt']):\n",
    "    df['bed_bath_ratio'] = df['bedroomcnt'] / (df['bathroomcnt'] + 1)\n",
    "if all(col in df.columns for col in ['garagetotalsqft', 'garagecarcnt']):\n",
    "    df['garage_area'] = df['garagetotalsqft'].fillna(0) + df['garagecarcnt'].fillna(0) * 200\n",
    "\n",
    "#Location clusters (safe)\n",
    "if all(col in df.columns for col in ['latitude', 'longitude']):\n",
    "    from sklearn.cluster import KMeans\n",
    "    coords = df[['latitude', 'longitude']].fillna(0)\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "    df['region_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# Handle NaNs: Numerical median, Categorical mode\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.drop(['parcelid', 'logerror', 'transaction_month', 'transaction_year', 'transaction_day'], errors='ignore')\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()\n",
    "    df[col] = df[col].fillna(mode_val[0] if not mode_val.empty else 'Unknown')\n",
    "\n",
    "#Label encode categoricals (safe)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "X = df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1, errors='ignore')\n",
    "y = df['logerror']\n",
    "\n",
    "print(\"Checking for duplicate columns...\")\n",
    "duplicate_cols = df.columns[df.columns.duplicated()].tolist()\n",
    "if duplicate_cols:\n",
    "    print(f\"Found duplicate columns: {duplicate_cols}\")\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "else:\n",
    "    print(\"No duplicate columns – All good!\")\n",
    "\n",
    "print(f\"Final columns count: {df.shape[1]}\")\n",
    "\n",
    "X = df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1, errors='ignore')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "#Cleaning up\n",
    "del df\n",
    "if 'coords' in locals():\n",
    "    del coords\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34056cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:18:55.793662Z",
     "iopub.status.busy": "2025-11-24T17:18:55.793050Z",
     "iopub.status.idle": "2025-11-24T17:25:02.423746Z",
     "shell.execute_reply": "2025-11-24T17:25:02.420070Z"
    },
    "papermill": {
     "duration": 366.670726,
     "end_time": "2025-11-24T17:25:02.445009",
     "exception": false,
     "start_time": "2025-11-24T17:18:55.774283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training LightGBM ===\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's l1: 0.06031\n",
      "LightGBM Local MAE: 0.060310\n",
      "LightGBM full train done!\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: LightGBM Training (Super Stable & Fast)\n",
    "print(\"=== Training LightGBM ===\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 20000,\n",
    "    'learning_rate': 0.008,\n",
    "    'num_leaves': 128,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.65,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='mae',\n",
    "    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(500)]\n",
    ")\n",
    "\n",
    "lgb_pred_test = lgb_model.predict(X_test)\n",
    "lgb_mae = mean_absolute_error(y_test, lgb_pred_test)\n",
    "print(f\"LightGBM Local MAE: {lgb_mae:.6f}\")\n",
    "\n",
    "# Full train pe retrain for final submission\n",
    "lgb_model.fit(X, y)\n",
    "lgb_full_pred = lgb_model.predict(X)\n",
    "print(\"LightGBM full train done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5c0c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:25:02.483570Z",
     "iopub.status.busy": "2025-11-24T17:25:02.483255Z",
     "iopub.status.idle": "2025-11-24T17:34:45.373751Z",
     "shell.execute_reply": "2025-11-24T17:34:45.372795Z"
    },
    "papermill": {
     "duration": 582.911556,
     "end_time": "2025-11-24T17:34:45.375138",
     "exception": false,
     "start_time": "2025-11-24T17:25:02.463582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TabNet\n",
      "epoch 0  | loss: 0.64333 | valid_mae: 24.08553|  0:00:06s\n",
      "epoch 10 | loss: 0.01055 | valid_mae: 0.07087 |  0:00:57s\n",
      "epoch 20 | loss: 0.01032 | valid_mae: 0.06321 |  0:01:48s\n",
      "epoch 30 | loss: 0.01022 | valid_mae: 0.06086 |  0:02:38s\n",
      "epoch 40 | loss: 0.01017 | valid_mae: 0.06068 |  0:03:28s\n",
      "epoch 50 | loss: 0.01016 | valid_mae: 0.0606  |  0:04:19s\n",
      "epoch 60 | loss: 0.01012 | valid_mae: 0.06088 |  0:05:10s\n",
      "epoch 70 | loss: 0.01011 | valid_mae: 0.06135 |  0:06:01s\n",
      "epoch 80 | loss: 0.01012 | valid_mae: 0.06117 |  0:06:51s\n",
      "epoch 90 | loss: 0.01011 | valid_mae: 0.06124 |  0:07:41s\n",
      "epoch 100| loss: 0.0101  | valid_mae: 0.06079 |  0:08:32s\n",
      "epoch 110| loss: 0.01009 | valid_mae: 0.06099 |  0:09:23s\n",
      "\n",
      "Early stopping occurred at epoch 110 with best_epoch = 50 and best_valid_mae = 0.0606\n",
      "TabNet Local MAE: 0.060598\n",
      "TabNet successfully trained!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training TabNet\")\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "y_test_2d  = y_test.values.reshape(-1, 1)\n",
    "\n",
    "tabnet = TabNetRegressor(\n",
    "    n_d=64, n_a=64,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\": 30, \"gamma\": 0.95},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax',\n",
    "    verbose=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "tabnet.fit(\n",
    "    X_train=X_train.values, \n",
    "    y_train=y_train_2d,                \n",
    "    eval_set=[(X_test.values, y_test_2d)],\n",
    "    eval_name=['valid'],\n",
    "    eval_metric=['mae'],\n",
    "    max_epochs=300,\n",
    "    patience=60,\n",
    "    batch_size=2048,\n",
    "    virtual_batch_size=256\n",
    ")\n",
    "\n",
    "tabnet_pred_test = tabnet.predict(X_test.values).flatten()\n",
    "tabnet_pred_full = tabnet.predict(X.values).flatten()\n",
    "\n",
    "print(f\"TabNet Local MAE: {mean_absolute_error(y_test, tabnet_pred_test):.6f}\")\n",
    "print(\"TabNet successfully trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87bac0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:34:45.414748Z",
     "iopub.status.busy": "2025-11-24T17:34:45.414007Z",
     "iopub.status.idle": "2025-11-24T17:40:48.457825Z",
     "shell.execute_reply": "2025-11-24T17:40:48.454746Z"
    },
    "papermill": {
     "duration": 363.08595,
     "end_time": "2025-11-24T17:40:48.480305",
     "exception": false,
     "start_time": "2025-11-24T17:34:45.394355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Second LightGBM\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[600]\tvalid_0's l1: 0.0602823\tvalid_0's l2: 0.00994164\n",
      "Second LGBM done! Local MAE: 0.039518\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Second LightGBM\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lgb2_params = {\n",
    "    'n_estimators': 20000,\n",
    "    'learning_rate': 0.009,\n",
    "    'num_leaves': 96,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 25,\n",
    "    'subsample': 0.82,\n",
    "    'colsample_bytree': 0.68,\n",
    "    'reg_alpha': 0.3,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': 101,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "lgb2 = lgb.LGBMRegressor(**lgb2_params)\n",
    "lgb2.fit(X_train, y_train,\n",
    "         eval_set=[(X_test, y_test)],\n",
    "         eval_metric='mae',\n",
    "         callbacks=[lgb.early_stopping(250)])\n",
    "\n",
    "lgb2_pred = lgb2.fit(X, y).predict(X)\n",
    "print(f\"Second LGBM done! Local MAE: {mean_absolute_error(y_test, lgb2.predict(X_test)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76fe2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T17:40:48.519777Z",
     "iopub.status.busy": "2025-11-24T17:40:48.519066Z",
     "iopub.status.idle": "2025-11-24T17:46:03.475932Z",
     "shell.execute_reply": "2025-11-24T17:46:03.475303Z"
    },
    "papermill": {
     "duration": 314.978362,
     "end_time": "2025-11-24T17:46:03.477296",
     "exception": false,
     "start_time": "2025-11-24T17:40:48.498934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOCAL CV MAE = 0.047130\n"
     ]
    }
   ],
   "source": [
    "lgb1_pred = lgb_model.predict(X)\n",
    "\n",
    "lgb2_pred = lgb2.predict(X)\n",
    "\n",
    "tabnet_pred = tabnet.predict(X.values).flatten()\n",
    "\n",
    "final_pred = (0.40 * lgb1_pred +\n",
    "              0.50 * tabnet_pred +\n",
    "              0.10 * lgb2_pred)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(f\"FINAL LOCAL CV MAE = {mean_absolute_error(y, final_pred):.6f}\")\n",
    "\n",
    "import pandas as pd\n",
    "sub = pd.read_csv('/kaggle/input/zillow-prize-1/sample_submission.csv')\n",
    "value = final_pred.mean()\n",
    "\n",
    "sub['201610'] = value\n",
    "sub['201611'] = value\n",
    "sub['201612'] = value\n",
    "sub['201710'] = value\n",
    "sub['201711'] = value\n",
    "sub['201712'] = value\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 860670,
     "sourceId": 6649,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1774.810202,
   "end_time": "2025-11-24T17:46:05.854024",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-24T17:16:31.043822",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
